# Method

This study was conducted on 100,000 technical articles pulled from dev.to, a technical blogging platform. The lexicon-based approaches include Valence Aware Dictionary and sEntiment Reasoner (VADER), TextBlob, SentiWordNet, and Affirmative Norms for English Words (ANEW). For the machine learning method, deep learning is explored.

## VADER

The Valence Aware Dictionary and Sentiment Reasoner, also known as VADER, is a lexicon-based approach that determines the level of emotions expressed in written communication. Its goal is to extract sentiments from social media updates, reviews, and informal writing, which might present difficulties for standard sentiment analysis approaches due to the usage of slang, emoticons, and language that depends strongly on context. It is known for its accuracy in capturing sentiments from these types of text, even when they contain slang, emoticons, and other informal language.
For VADER to function, a dictionary of words and their corresponding sentiment scores must initially be created. The dictionary of words is known as the dictionary. The scores are determined by some variables, such as the definition of the word, its usage in context, and its part of speech. Once the dictionary has been established, VADER uses the text to identify the overall sentiment by applying a set of rules. These rules take into account the sentiment scores of the individual words, as well as the order in which they appear.
VADER bases its operations on the idea that words have an emotional valence, which means that they can be used to express either positive or negative sentiments. It takes into account the sentiment-altering effects of punctuation, capitalization, and degree modifiers (such as "very" or "extremely") in addition to the meaning of individual words.
Each word in the pre-constructed vocabulary used by VADER is given a sentiment value between -1 and +1. numbers below zero imply neutrality, while numbers above zero indicate favourable sentiment.
In addition to VADER's accuracy, it is also known for its flexibility. It can be used to analyze text in a variety of languages and domains, and it can be customized to specific applications. VADER is a popular choice for sentiment analysis in a variety of industries, including social media, marketing, and customer service.
In this study, the NLP Vader class used was SentimentIntensityAnalyzer which provides a sentiment intensity score for text-based content. An instance of the SentimentIntensityAnalyzer class is created which assigns Vader's predetermined vocabulary to the lexicon file parameter by default. The polarity scores method is called on the object. The method takes in text-based content via the text parameter. The method returns a float for the sentiment score based on the text input, where the values are positive and negative valence. Internally, an instance of the SentiText class is initialized which would remove leading and trailing punctuations using a regular expression. The punctuation to be removed is defined by the Vader Library. The valence, and sentiment, are computed on several layers by first splitting the results after stripping the punctuations into a iterable list. For each item in the list, the item is checked if it is a booster word. Boosters are a predefined vocabulary with fixed booster scores. These booster scores are the empirically derived mean sentiment intensity rating. If the item is not a booster word, the items index, the punctuation stripped text and the sentiment list. The item is checked against Vader’s vocabulary to compute the item’s valence. After a “but” check is done. It is a way to check if the word, “but” was in the punctuation-stripped text and then halves the valence score of all the words that come before the “but” word. It would double the valence score for all that comes after the “but” word.
Finally, the valence score is computed. The sum computed is altered by adding the emphasis provided by punctuation. The score is normalized to lie between -1 and +1 by computing the ratio of the score over the square root of the sum of the square of the current sentiment score and a constant, alpha, which approximates the maximum expected value. This score is known as the compound score. Based on the compound value, the positive, negative and neutral score is computed and returned as a dictionary, sentiment_dict, of with keys, compound, neg, pos, and neu with floating point values.

## TextBlob

TextBlob is a textual data processor that provides a consistent API for delving into typical natural language processing (NLP) activities including part-of-speech tagging, noun phrase extraction, sentiment analysis, and more.
TextBlob uses labelled data to build models that can perform sentiment analysis and classification. This means that TextBlob first takes a set of text samples and assigns each sample text a label indicating its emotional tone. It then uses these labelled samples to train a model that can predict the emotional tone of new text samples.
TextBlob evaluates the polarity of each piece of text with scores ranging from -1 to +1. Scores close to 0 represent neutral sentiment, negative scores indicate negative sentiment and positive scores indicate positive sentiment.
The text data is analyzed by the machine learning model to find patterns associated with particular sentiments. These patterns may consist of language clues, words, and phrases that are indicative of positive, negative, or neutral feelings.
In this study, an instance of the TextBlob class passing the raw text to be analyzed. The sentiment property is called on the TextBlob instance which returns a named tuple of the form, polarity and subjectivity. The polarity is a float with the range of -1 and 1, the subjectivity is a value in the range of 0 and 1, From this tuple, the polarity is selected. This is computation based is on the combination of several predefined vocabulary.

## SentiWordNet

SentiWordNet is a lexical tool for opinion mining. Each synset of WordNet is given one of three sentiment scores based on objectivity, negativity, and positivity. This is based on the WordNetCorpusReader. WordNet is a lexical database for the English language that provides details on word relationships, meanings, and other lexical data. Users can access and use the WordNet database programmatically thanks to the WordNetCorpusReader. Users can use it to look up synonyms and antonyms for a specific word. To comprehend the variety of sentiments connected to a given word, this can be useful in sentiment analysis. The reader provides users with the option to look up a word's hyponyms and hypernyms. Analyzing the context and emotion of words in a text can benefit from this hierarchical knowledge. The reader provides users with the option to look up a word's hyponyms and hypernyms. Analyzing the context and emotion of words in a text can benefit from this hierarchical knowledge.
In this study, a given text is split into a list of words, and each word is passed to the WordNetCorpusReader method, synset, which will load all the synonymous sets of the given word. The vocabulary for the word is loaded to map the word to its synonymous set (synset). Each item in the synset could be just a word a times. The synset has a method, name, which when called will return the offset and position of the word (lemma) which is then used by the LazyCorpusLoader method senti_synset to get the positive and negative scores. The difference between the positive and negative scores is then used as the sentiment score.

## ANEW

Affective Norms for English Words assigns pre-computed sentiment scores to English words. The sentiment scores range from -5 to +5, where negative scores indicate a negative sentiment, positive scores indicate a positive sentiment and a score of 0 implies a neutral sentiment for that word. Some word or phrase such as bastard
, catastrophic, can't stand, accident, affected, some kind, solve, smile, soothe, stunning and thrilled are ranked from -5 to 5 respectively, with other words and phrase with the same sentiment scores. ANEW has a file which are being scored.
In this study, a file that contains the ANEW is read and a vocabulary of words with their sentiment score is created. The text provided is split into list words and iterated over the vocabulary to get the sentiment score.

## Deep learning

A branch of machine learning known as "deep learning" makes use of artificial neural networks to learn from data. Deep learning models are trained on a vast quantity of data. They can see patterns and make predictions that would be challenging for people to make. Sentiment analysis is only one of the many uses for deep learning models
In this study, Convolutional Neural Networks (CNNs) was used. Convolutional Neural Networks (CNNs) are a subset of deep learning models that were originally developed for image processing applications. However, because of their design and capacity to recognize patterns and hierarchical structures, they have been used for text analysis. application of CNN, certain methodolgies including but not limited to Word Embeddings. Word Embeddings can achieved using methods like Word2Vec, GloVe, or FastText, where dense vector representations of the text's words are created. These vector representations of words capture underlying logical relationships.
The dataset to be used after they were read from the source file, were assigned a sentiment score using Vader. After a technique called Upsampling is applied on the dataset. Upsampling is used to balance the classes in the dataset by dupliating the minority examples so that they have the same numbrt of examples as the majority. This technique improves the accuracy of the model since the dataset becomes balanced. Upsampling is also, another way to prevent overfitting. The dataset is split into training and testing dataset then in any of the dataset, any row that is designated as NA, Not Any, is replace with an empty string.
An instance of the of keras’ text processing Tokenizer class is created. The Tokenizer class enables the vectorization of text corpora by converting each text into either a series of integers (each integer representing the index of a token in a dictionary) or a vector with a binary coefficient for each token based on word count or based on Term Frequency–Inverse Document Frequency (tf-idf), a measure that is used to compute the importance of a word in a document. By default, all punctuation is deleted from the texts, leaving only space-separated word sequences, which may contain the apostrophe character. The lists of tokens from these sequences are then separated. After that, they'll be vectorized or indexed. An index that is reserved and won't be given to any words is "0".
Each item in the training and testing dataset is transformed into a sequence of integers using the texts_to_sequences method of the Tokenizer class instance. The transformed data is padded by keras’ sequence processing method, pad_sequences. This method converts a list of sequences (lists of numbers) with length 'num_samples' into a 2D Numpy array with shape '(num_samples, num_timesteps)'. 'num_timesteps' is the longest sequence in the list, or the'maxlen' parameter, if one is given.
An instance of an encoding label with the sklearn’s processing LabelEncoding class. The y_train dataset is transformed using the fit_transform method on the LabelEncoding class instance and y_test dataset, using the transform method to normalize the encoding.
A sequential model in Keras is created, which is a type of neural network that consists of a linear stack of layers, providing training and inference features on the model.
The first layer in the model is an Embedding layer. The Embedding layer maps each word in the vocabulary to a vector of n dimensions, the size of the vector that the Embedding layer maps each word to. This allows the model to learn the meaning of words in the context of the text. The next layer is a Dense layer with 3 neurons. The Dense layer is a fully connected layer that connects all of the neurons in the previous layer to all of the neurons in the current layer. The activation function for the Dense layer is softmax, which is a function that outputs a probability distribution over the 3 classes.
Two more layers, the Conv1D and MaxPooling1D layers are added. These layers are used for text classification tasks. The Conv1D layer applies a 1D convolution to the input sequence. The MaxPooling1D layer applies a max pooling operation to the output of the Conv1D layer.
The next two layers, added are Dense layers with 16 neurons each. These layers are used to learn more complex features from the text. The activation function for these layers is ReLU, which is a function that outputs the positive part of the input. The next layer is a GlobalMaxPooling1D layer. This layer applies a global max pooling operation to the output of the previous layer. This reduces the dimensionality of the output and allows the model to learn more general features.
The final layer is a Dense layer with 1 neuron. The activation function for this layer is sigmoid, which outputs a value between 0 and 1. This value represents the probability that the input text belongs to the positive class.
The model is compiled using the binary_crossentropy loss function and the Adam optimizer. The metrics for the model are accuracy and loss. The model summary shows the details of the model, such as the number of layers, the number of neurons in each layer, and the activation functions for each layer.
The EarlyStopping Keras callback class object is created. If the validation loss does not decrease for a predetermined amount of epochs, it is used to halt the training of a model. The monitor, mode, verbose, and patience options are sent to the constructor. The monitor parameter specifies the metric to monitor as well as the location of the validation loss monitoring. The mode represents the metric's statistical mode. Since the mode is set to a minimum, training will end if the validation loss does not reduce. The verbose setting specifies how much data should be logged. Usually, this is an integer. The number of epochs without improvement before training is ended is referred to as the patience parameter. When using the EarlyStopping callback, the model's training is terminated if the validation loss does not decrease after a predetermined number of epochs. Overfitting is an issue that arises when a model learns the training data too well and is unable to generalize to new data. This can help to prevent overfitting.
When the validation accuracy increases, a Keras callback called ModelCheckpoint can be used to save the model to a file after each epoch. The constructor requires the options filepath, monitor, mode, verbose, and save_best_only. The model's saving file is identified by its path, filepath. This metric has to be watched. Here, the validation accuracy serves as the statistic. The metric's mode is mode. The callback will save the model in this instance if the validation accuracy rises because the mode is set to "max." There are several levels of verbosity. One can choose to save either the best model or all models using the save_best_only option. When the validation accuracy increases while the ModelCheckpoint callback is in use, the model will be saved to a file after each epoch. This can help to prevent overfitting and preserve the best model during training. The save model is then used for prediction later.

## Justification

The techniques used in the selected approaches span different areas, from lexicon-based methods such as VADER, TextBlob, Affective Norms, and SentiWordNet to deep learning with CNNs. This variety is advantageous since the style and content of technical publications can vary. While CNNs are capable of capturing more complicated patterns and hierarchical structures in the text, lexicon-based approaches are appropriate for simple sentiment analysis of individual words or phrases. Additionally, the method investigates the effects of taking synonyms and antonyms into account in sentiment analysis, which can be very helpful when dealing with domain-specific language in technical publications. This is done by adding WordNetCorpusReader for synonym and antonym analysis. Combining these techniques offers a solid strategy for handling the sentiment analysis task in technical articles.
